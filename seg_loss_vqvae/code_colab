{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIURzvp9KfYA3FlDHM209/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"4454b8626e1447f1b947f837cf52d8a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cacf86050c72414fa7f2db99b454e905","IPY_MODEL_010016a86ea141d4b913138eeca0ea0b","IPY_MODEL_a567a89e111a484681f8f023bb7ae53c"],"layout":"IPY_MODEL_b6d0620a8ad64850870908befa8d2d55"}},"cacf86050c72414fa7f2db99b454e905":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7141122592647e59041fa9b9954eb0a","placeholder":"​","style":"IPY_MODEL_b7d4a6f0d7504e5984e11c88d10b7abc","value":"100%"}},"010016a86ea141d4b913138eeca0ea0b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c9e3fb7089b4a85bafdd5ac7cb85159","max":46830571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14065f083e58469da3b6b0d571a71c84","value":46830571}},"a567a89e111a484681f8f023bb7ae53c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebd94c6a7efa4d5e89b8399c3e7d68fc","placeholder":"​","style":"IPY_MODEL_77f0a9756b164e05a81cb07fe20fa13e","value":" 44.7M/44.7M [00:00&lt;00:00, 159MB/s]"}},"b6d0620a8ad64850870908befa8d2d55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7141122592647e59041fa9b9954eb0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7d4a6f0d7504e5984e11c88d10b7abc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c9e3fb7089b4a85bafdd5ac7cb85159":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14065f083e58469da3b6b0d571a71c84":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ebd94c6a7efa4d5e89b8399c3e7d68fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77f0a9756b164e05a81cb07fe20fa13e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gIB6EJEGWu8","executionInfo":{"status":"ok","timestamp":1670102007722,"user_tz":360,"elapsed":14816,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}},"outputId":"0405877b-ec1b-4e3f-9849-f51306e53a10"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["cd gdrive/MyDrive/UChicago/Academics/Computer Science/TTIC 31230/Final Project/vqvae_modified"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYtOajkijolV","executionInfo":{"status":"ok","timestamp":1670102011442,"user_tz":360,"elapsed":1916,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}},"outputId":"6cd7eb2a-11c3-4c23-afca-a84dc2015c57"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/1X28loE-F3vl4_iI9qGeFmIZC-kNS-9iV/UChicago/Academics/Computer Science/TTIC 31230/Final Project/vqvae_modified\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.autograd import Function\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.distributions.normal import Normal\n","from torch.distributions import kl_divergence\n","from torchvision import transforms, datasets\n","from torchvision.utils import save_image, make_grid\n","from torchvision.models import resnet18\n","import numpy as np\n","from matplotlib import pyplot as plt"],"metadata":{"id":"OZq50ygeas4U","executionInfo":{"status":"ok","timestamp":1670103058312,"user_tz":360,"elapsed":122,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# functions.py\n","class VectorQuantization(Function):\n","    @staticmethod\n","    def forward(ctx, inputs, codebook):\n","        with torch.no_grad():\n","            embedding_size = codebook.size(1)\n","            inputs_size = inputs.size()\n","            inputs_flatten = inputs.view(-1, embedding_size)\n","\n","            codebook_sqr = torch.sum(codebook ** 2, dim=1)\n","            inputs_sqr = torch.sum(inputs_flatten ** 2, dim=1, keepdim=True)\n","\n","            # Compute the distances to the codebook\n","            distances = torch.addmm(codebook_sqr + inputs_sqr,\n","                inputs_flatten, codebook.t(), alpha=-2.0, beta=1.0)\n","\n","            _, indices_flatten = torch.min(distances, dim=1)\n","            indices = indices_flatten.view(*inputs_size[:-1])\n","            ctx.mark_non_differentiable(indices)\n","\n","            return indices\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        raise RuntimeError('Trying to call `.grad()` on graph containing '\n","            '`VectorQuantization`. The function `VectorQuantization` '\n","            'is not differentiable. Use `VectorQuantizationStraightThrough` '\n","            'if you want a straight-through estimator of the gradient.')\n","\n","vq = VectorQuantization()\n","\n","class VectorQuantizationStraightThrough(Function):\n","    @staticmethod\n","    def forward(ctx, inputs, codebook):\n","        indices = vq.apply(inputs, codebook)\n","        indices_flatten = indices.view(-1)\n","        ctx.save_for_backward(indices_flatten, codebook)\n","        ctx.mark_non_differentiable(indices_flatten)\n","\n","        codes_flatten = torch.index_select(codebook, dim=0,\n","            index=indices_flatten)\n","        codes = codes_flatten.view_as(inputs)\n","\n","        return (codes, indices_flatten)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output, grad_indices):\n","        grad_inputs, grad_codebook = None, None\n","\n","        if ctx.needs_input_grad[0]:\n","            # Straight-through estimator\n","            grad_inputs = grad_output.clone()\n","        if ctx.needs_input_grad[1]:\n","            # Gradient wrt. the codebook\n","            indices, codebook = ctx.saved_tensors\n","            embedding_size = codebook.size(1)\n","\n","            grad_output_flatten = (grad_output.contiguous()\n","                                              .view(-1, embedding_size))\n","            grad_codebook = torch.zeros_like(codebook)\n","            grad_codebook.index_add_(0, indices, grad_output_flatten)\n","\n","        return (grad_inputs, grad_codebook)"],"metadata":{"id":"oPhuv7kgJ0xj","executionInfo":{"status":"ok","timestamp":1670102017580,"user_tz":360,"elapsed":127,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["vq_st = VectorQuantizationStraightThrough()"],"metadata":{"id":"n8jd0zufKghu","executionInfo":{"status":"ok","timestamp":1670102019303,"user_tz":360,"elapsed":3,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["num_classes = 10\n","resnet = resnet18(pretrained=True)\n","resnet.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n","resnet_ = list(resnet.children())[:-2]\n","resnet_[3] = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","classifier = nn.Conv2d(512, num_classes, 1)\n","torch.nn.init.kaiming_normal_(classifier.weight)\n","resnet_.append(classifier)\n","resnet_.append(nn.Upsample(size=32, mode='bilinear', align_corners=False))\n","tiny_resnet = nn.Sequential(*resnet_)\n","seg_model = nn.DataParallel(tiny_resnet).cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159,"referenced_widgets":["4454b8626e1447f1b947f837cf52d8a0","cacf86050c72414fa7f2db99b454e905","010016a86ea141d4b913138eeca0ea0b","a567a89e111a484681f8f023bb7ae53c","b6d0620a8ad64850870908befa8d2d55","d7141122592647e59041fa9b9954eb0a","b7d4a6f0d7504e5984e11c88d10b7abc","3c9e3fb7089b4a85bafdd5ac7cb85159","14065f083e58469da3b6b0d571a71c84","ebd94c6a7efa4d5e89b8399c3e7d68fc","77f0a9756b164e05a81cb07fe20fa13e"]},"id":"lcE1OcxZ1wXE","executionInfo":{"status":"ok","timestamp":1670102441206,"user_tz":360,"elapsed":5548,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}},"outputId":"3586f555-af78-4f8d-ce16-f5aabf964582"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/44.7M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4454b8626e1447f1b947f837cf52d8a0"}},"metadata":{}}]},{"cell_type":"code","source":["seg_model.load_state_dict(torch.load('./models/seg_model.pt'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fmj_K4d3BtZ","executionInfo":{"status":"ok","timestamp":1670102444898,"user_tz":360,"elapsed":928,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}},"outputId":"d0f8eac1-19ed-4f1e-b02a-5a4c8687bcc7"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["def attention(x):\n","    return torch.sigmoid(torch.logsumexp(x, 1, keepdim=True))"],"metadata":{"id":"WzRg_7Rk3P7Q","executionInfo":{"status":"ok","timestamp":1670102509531,"user_tz":360,"elapsed":118,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# modules.py\n","def to_scalar(arr):\n","    if type(arr) == list:\n","        return [x.item() for x in arr]\n","    else:\n","        return arr.item()\n","\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        try:\n","            nn.init.xavier_uniform_(m.weight.data)\n","            m.bias.data.fill_(0)\n","        except AttributeError:\n","            print(\"Skipping initialization of \", classname)\n","\n","\n","class VQEmbedding(nn.Module):\n","    def __init__(self, K, D):\n","        super().__init__()\n","        self.embedding = nn.Embedding(K, D)\n","        self.embedding.weight.data.uniform_(-1./K, 1./K)\n","\n","    def forward(self, z_e_x):\n","        z_e_x_ = z_e_x.permute(0, 2, 3, 1).contiguous()\n","        latents = vq.apply(z_e_x_, self.embedding.weight)\n","        return latents\n","\n","    def straight_through(self, z_e_x):\n","        z_e_x_ = z_e_x.permute(0, 2, 3, 1).contiguous()\n","        z_q_x_, indices = vq_st.apply(z_e_x_, self.embedding.weight.detach())\n","        z_q_x = z_q_x_.permute(0, 3, 1, 2).contiguous()\n","\n","        z_q_x_bar_flatten = torch.index_select(self.embedding.weight,\n","            dim=0, index=indices)\n","        z_q_x_bar_ = z_q_x_bar_flatten.view_as(z_e_x_)\n","        z_q_x_bar = z_q_x_bar_.permute(0, 3, 1, 2).contiguous()\n","\n","        return z_q_x, z_q_x_bar\n","\n","\n","class ResBlock(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            nn.ReLU(True),\n","            nn.Conv2d(dim, dim, 3, 1, 1),\n","            nn.BatchNorm2d(dim),\n","            nn.ReLU(True),\n","            nn.Conv2d(dim, dim, 1),\n","            nn.BatchNorm2d(dim)\n","        )\n","\n","    def forward(self, x):\n","        return x + self.block(x)\n","\n","\n","class VectorQuantizedVAE(nn.Module):\n","    def __init__(self, input_dim, dim, K=512):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(input_dim, dim, 4, 2, 1),\n","            nn.BatchNorm2d(dim),\n","            nn.ReLU(True),\n","            nn.Conv2d(dim, dim, 4, 2, 1),\n","            ResBlock(dim),\n","            ResBlock(dim),\n","        )\n","\n","        self.codebook = VQEmbedding(K, dim)\n","\n","        self.decoder = nn.Sequential(\n","            ResBlock(dim),\n","            ResBlock(dim),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(dim, dim, 4, 2, 1),\n","            nn.BatchNorm2d(dim),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(dim, input_dim-1, 4, 2, 1), # minus 1 because we subtract the segmentation channel\n","            nn.Tanh()\n","        )\n","\n","        self.apply(weights_init)\n","\n","    def encode(self, x):\n","        z_e_x = self.encoder(x)\n","        latents = self.codebook(z_e_x)\n","        return latents\n","\n","    def decode(self, latents):\n","        z_q_x = self.codebook.embedding(latents).permute(0, 3, 1, 2)  # (B, D, H, W)\n","        x_tilde = self.decoder(z_q_x)\n","        return x_tilde\n","\n","    def forward(self, x):\n","        z_e_x = self.encoder(x)\n","        z_q_x_st, z_q_x = self.codebook.straight_through(z_e_x)\n","        x_tilde = self.decoder(z_q_x_st)\n","        return x_tilde, z_e_x, z_q_x\n","\n","\n","class GatedActivation(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        x, y = x.chunk(2, dim=1)\n","        return F.tanh(x) * F.sigmoid(y)\n","\n","\n","class GatedMaskedConv2d(nn.Module):\n","    def __init__(self, mask_type, dim, kernel, residual=True, n_classes=10):\n","        super().__init__()\n","        assert kernel % 2 == 1, print(\"Kernel size must be odd\")\n","        self.mask_type = mask_type\n","        self.residual = residual\n","\n","        self.class_cond_embedding = nn.Embedding(\n","            n_classes, 2 * dim\n","        )\n","\n","        kernel_shp = (kernel // 2 + 1, kernel)  # (ceil(n/2), n)\n","        padding_shp = (kernel // 2, kernel // 2)\n","        self.vert_stack = nn.Conv2d(\n","            dim, dim * 2,\n","            kernel_shp, 1, padding_shp\n","        )\n","\n","        self.vert_to_horiz = nn.Conv2d(2 * dim, 2 * dim, 1)\n","\n","        kernel_shp = (1, kernel // 2 + 1)\n","        padding_shp = (0, kernel // 2)\n","        self.horiz_stack = nn.Conv2d(\n","            dim, dim * 2,\n","            kernel_shp, 1, padding_shp\n","        )\n","\n","        self.horiz_resid = nn.Conv2d(dim, dim, 1)\n","\n","        self.gate = GatedActivation()\n","\n","    def make_causal(self):\n","        self.vert_stack.weight.data[:, :, -1].zero_()  # Mask final row\n","        self.horiz_stack.weight.data[:, :, :, -1].zero_()  # Mask final column\n","\n","    def forward(self, x_v, x_h, h):\n","        if self.mask_type == 'A':\n","            self.make_causal()\n","\n","        h = self.class_cond_embedding(h)\n","        h_vert = self.vert_stack(x_v)\n","        h_vert = h_vert[:, :, :x_v.size(-1), :]\n","        out_v = self.gate(h_vert + h[:, :, None, None])\n","\n","        h_horiz = self.horiz_stack(x_h)\n","        h_horiz = h_horiz[:, :, :, :x_h.size(-2)]\n","        v2h = self.vert_to_horiz(h_vert)\n","\n","        out = self.gate(v2h + h_horiz + h[:, :, None, None])\n","        if self.residual:\n","            out_h = self.horiz_resid(out) + x_h\n","        else:\n","            out_h = self.horiz_resid(out)\n","\n","        return out_v, out_h\n","\n","\n","class GatedPixelCNN(nn.Module):\n","    def __init__(self, input_dim=256, dim=64, n_layers=15, n_classes=10):\n","        super().__init__()\n","        self.dim = dim\n","\n","        # Create embedding layer to embed input\n","        self.embedding = nn.Embedding(input_dim, dim)\n","\n","        # Building the PixelCNN layer by layer\n","        self.layers = nn.ModuleList()\n","\n","        # Initial block with Mask-A convolution\n","        # Rest with Mask-B convolutions\n","        for i in range(n_layers):\n","            mask_type = 'A' if i == 0 else 'B'\n","            kernel = 7 if i == 0 else 3\n","            residual = False if i == 0 else True\n","\n","            self.layers.append(\n","                GatedMaskedConv2d(mask_type, dim, kernel, residual, n_classes)\n","            )\n","\n","        # Add the output layer\n","        self.output_conv = nn.Sequential(\n","            nn.Conv2d(dim, 512, 1),\n","            nn.ReLU(True),\n","            nn.Conv2d(512, input_dim, 1)\n","        )\n","\n","        self.apply(weights_init)\n","\n","    def forward(self, x, label):\n","        shp = x.size() + (-1, )\n","        x = self.embedding(x.view(-1)).view(shp)  # (B, H, W, C)\n","        x = x.permute(0, 3, 1, 2)  # (B, C, W, W)\n","\n","        x_v, x_h = (x, x)\n","        for i, layer in enumerate(self.layers):\n","            x_v, x_h = layer(x_v, x_h, label)\n","\n","        return self.output_conv(x_h)\n","\n","    def generate(self, label, shape=(8, 8), batch_size=64):\n","        param = next(self.parameters())\n","        x = torch.zeros(\n","            (batch_size, *shape),\n","            dtype=torch.int64, device=param.device\n","        )\n","\n","        for i in range(shape[0]):\n","            for j in range(shape[1]):\n","                logits = self.forward(x, label)\n","                probs = F.softmax(logits[:, :, i, j], -1)\n","                x.data[:, i, j].copy_(\n","                    probs.multinomial(1).squeeze().data\n","                )\n","        return x"],"metadata":{"id":"xKdc2Pv-JwEr","executionInfo":{"status":"ok","timestamp":1670104477022,"user_tz":360,"elapsed":114,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["def train(data_loader, model, optimizer):\n","  for images, _ in data_loader:\n","    images = images.to('cuda')\n","    seg_channel = seg_model(images)\n","    seg_channel = attention(seg_channel)\n","    seg_imgs = torch.cat((images, seg_channel), dim=1)\n","\n","    optimizer.zero_grad()\n","    x_tilde, z_e_x, z_q_x = model(seg_imgs)\n","\n","    # Reconstruction loss\n","    loss_recons = F.mse_loss(x_tilde, images)\n","    # Vector quantization objective\n","    loss_vq = F.mse_loss(z_q_x, z_e_x.detach())\n","    # Commitment objective\n","    loss_commit = F.mse_loss(z_e_x, z_q_x.detach())\n","\n","    loss = loss_recons + loss_vq + loss_commit\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","def test(data_loader, model):\n","  with torch.no_grad():\n","    loss_recons, loss_vq = 0., 0.\n","    for images, _ in data_loader:\n","      images = images.to('cuda')\n","      seg_channel = seg_model(images)\n","      seg_channel = attention(seg_channel)\n","      seg_imgs = torch.cat((images, seg_channel), dim=1)\n","\n","      x_tilde, z_e_x, z_q_x = model(seg_imgs)\n","      loss_recons += F.mse_loss(x_tilde, images)\n","      loss_vq += F.mse_loss(z_q_x, z_e_x)\n","\n","    loss_recons /= len(data_loader)\n","    loss_vq /= len(data_loader)\n","\n","  return loss_recons.item(), loss_vq.item()"],"metadata":{"id":"rmwtml0GZB6f","executionInfo":{"status":"ok","timestamp":1670104789645,"user_tz":360,"elapsed":306,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","train_dataset = datasets.CIFAR10(root='./data/cifar10/', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data/cifar10', train=False, transform=transform)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=False, pin_memory=True)\n","valid_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, \n","                                           drop_last=True, pin_memory=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ix2Um20IZB9G","executionInfo":{"status":"ok","timestamp":1670102753903,"user_tz":360,"elapsed":2435,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}},"outputId":"1a3f3616-2828-4e8d-ae55-cfa3c21c46dc"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["seg_model.eval()\n","for i, (x_batch, label) in enumerate(train_loader):\n","  if i == 50:\n","    x = x_batch.cuda()\n","    seg_out = seg_model(x)\n","    attn = attention(seg_out)\n","    break"],"metadata":{"id":"wrAJ39_Y7EGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# making sure the segmentation model works\n","attn = torch.reshape(attn, (128, 32, 32))\n","attn_np = attn.cpu().detach().numpy()\n","plt.imshow(attn_np[5], cmap='gray')"],"metadata":{"id":"J8wELhrV7EKS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.permute(x, (0, 2, 3, 1))\n","x_np = x.cpu().detach().numpy()\n","plt.imshow(x_np[5])"],"metadata":{"id":"bPeMYI7q7EMZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_dim = 4\n","hidden_dim = 256\n","num_epochs = 20\n","\n","model = VectorQuantizedVAE(input_dim, hidden_dim).to('cuda')\n","optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n","\n","best_loss = -1.\n","for epoch in range(num_epochs):\n","    train(train_loader, model, optimizer)\n","    loss, _ = test(valid_loader, model)\n","    print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"id":"qStTBCZfZJaO","executionInfo":{"status":"error","timestamp":1670105568816,"user_tz":360,"elapsed":775256,"user":{"displayName":"Dang Nguyen","userId":"07242743257633984321"}},"outputId":"0daee527-78e3-4bfe-d981-710f7745077f"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["0.055679094046354294\n","0.05337127298116684\n","0.05184309929609299\n","0.05054956674575806\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-50090bae0a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-52-da47b62ea1e9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_loader, model, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mseg_channel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mseg_channel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_channel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# vqvae.py\n","def train(data_loader, model, optimizer, args, writer):\n","    for images, _ in data_loader:\n","        images = images.to(args.device)\n","\n","        optimizer.zero_grad()\n","        x_tilde, z_e_x, z_q_x = model(images)\n","\n","        # Reconstruction loss\n","        loss_recons = F.mse_loss(x_tilde, images)\n","        # Vector quantization objective\n","        loss_vq = F.mse_loss(z_q_x, z_e_x.detach())\n","        # Commitment objective\n","        loss_commit = F.mse_loss(z_e_x, z_q_x.detach())\n","\n","        loss = loss_recons + loss_vq + args.beta * loss_commit\n","        loss.backward()\n","\n","        # Logs\n","        writer.add_scalar('loss/train/reconstruction', loss_recons.item(), args.steps)\n","        writer.add_scalar('loss/train/quantization', loss_vq.item(), args.steps)\n","\n","        optimizer.step()\n","        args.steps += 1\n","\n","def test(data_loader, model, args, writer):\n","    with torch.no_grad():\n","        loss_recons, loss_vq = 0., 0.\n","        for images, _ in data_loader:\n","            images = images.to(args.device)\n","            x_tilde, z_e_x, z_q_x = model(images)\n","            loss_recons += F.mse_loss(x_tilde, images)\n","            loss_vq += F.mse_loss(z_q_x, z_e_x)\n","\n","        loss_recons /= len(data_loader)\n","        loss_vq /= len(data_loader)\n","\n","    # Logs\n","    writer.add_scalar('loss/test/reconstruction', loss_recons.item(), args.steps)\n","    writer.add_scalar('loss/test/quantization', loss_vq.item(), args.steps)\n","\n","    return loss_recons.item(), loss_vq.item()\n","\n","def generate_samples(images, model, args):\n","    with torch.no_grad():\n","        images = images.to(args.device)\n","        x_tilde, _, _ = model(images)\n","    return x_tilde\n","\n","def main(args):\n","    writer = SummaryWriter('./logs/{0}'.format(args.output_folder))\n","    save_filename = './models/{0}'.format(args.output_folder)\n","\n","    if args.dataset in ['mnist', 'fashion-mnist', 'cifar10']:\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","        if args.dataset == 'mnist':\n","            # Define the train & test datasets\n","            train_dataset = datasets.MNIST(args.data_folder, train=True,\n","                download=True, transform=transform)\n","            test_dataset = datasets.MNIST(args.data_folder, train=False,\n","                transform=transform)\n","            num_channels = 1\n","        elif args.dataset == 'fashion-mnist':\n","            # Define the train & test datasets\n","            train_dataset = datasets.FashionMNIST(args.data_folder,\n","                train=True, download=True, transform=transform)\n","            test_dataset = datasets.FashionMNIST(args.data_folder,\n","                train=False, transform=transform)\n","            num_channels = 1\n","        elif args.dataset == 'cifar10':\n","            # Define the train & test datasets\n","            train_dataset = datasets.CIFAR10(args.data_folder,\n","                train=True, download=True, transform=transform)\n","            test_dataset = datasets.CIFAR10(args.data_folder,\n","                train=False, transform=transform)\n","            num_channels = 3\n","        valid_dataset = test_dataset\n","    elif args.dataset == 'miniimagenet':\n","        transform = transforms.Compose([\n","            transforms.RandomResizedCrop(128),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","        # Define the train, valid & test datasets\n","        train_dataset = MiniImagenet(args.data_folder, train=True,\n","            download=True, transform=transform)\n","        valid_dataset = MiniImagenet(args.data_folder, valid=True,\n","            download=True, transform=transform)\n","        test_dataset = MiniImagenet(args.data_folder, test=True,\n","            download=True, transform=transform)\n","        num_channels = 3\n","\n","    # Define the data loaders\n","    train_loader = torch.utils.data.DataLoader(train_dataset,\n","        batch_size=args.batch_size, shuffle=False,\n","        num_workers=args.num_workers, pin_memory=True)\n","    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n","        batch_size=args.batch_size, shuffle=False, drop_last=True,\n","        num_workers=args.num_workers, pin_memory=True)\n","    test_loader = torch.utils.data.DataLoader(test_dataset,\n","        batch_size=16, shuffle=True)\n","\n","    # Fixed images for Tensorboard\n","    fixed_images, _ = next(iter(test_loader))\n","    fixed_grid = make_grid(fixed_images, nrow=8, range=(-1, 1), normalize=True)\n","    writer.add_image('original', fixed_grid, 0)\n","\n","    model = VectorQuantizedVAE(num_channels, args.hidden_size, args.k).to(args.device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","    # Generate the samples first once\n","    reconstruction = generate_samples(fixed_images, model, args)\n","    grid = make_grid(reconstruction.cpu(), nrow=8, range=(-1, 1), normalize=True)\n","    writer.add_image('reconstruction', grid, 0)\n","\n","    best_loss = -1.\n","    for epoch in range(args.num_epochs):\n","        train(train_loader, model, optimizer, args, writer)\n","        loss, _ = test(valid_loader, model, args, writer)\n","\n","        reconstruction = generate_samples(fixed_images, model, args)\n","        grid = make_grid(reconstruction.cpu(), nrow=8, range=(-1, 1), normalize=True)\n","        writer.add_image('reconstruction', grid, epoch + 1)\n","\n","        if (epoch == 0) or (loss < best_loss):\n","            best_loss = loss\n","            with open('{0}/best.pt'.format(save_filename), 'wb') as f:\n","                torch.save(model.state_dict(), f)\n","        with open('{0}/model_{1}.pt'.format(save_filename, epoch + 1), 'wb') as f:\n","            torch.save(model.state_dict(), f)\n","\n","if __name__ == '__main__':\n","    import argparse\n","    import os\n","    import multiprocessing as mp\n","\n","    parser = argparse.ArgumentParser(description='VQ-VAE')\n","\n","    # General\n","    parser.add_argument('--data-folder', type=str,\n","        help='name of the data folder')\n","    parser.add_argument('--dataset', type=str,\n","        help='name of the dataset (mnist, fashion-mnist, cifar10, miniimagenet)')\n","\n","    # Latent space\n","    parser.add_argument('--hidden-size', type=int, default=256,\n","        help='size of the latent vectors (default: 256)')\n","    parser.add_argument('--k', type=int, default=512,\n","        help='number of latent vectors (default: 512)')\n","\n","    # Optimization\n","    parser.add_argument('--batch-size', type=int, default=128,\n","        help='batch size (default: 128)')\n","    parser.add_argument('--num-epochs', type=int, default=100,\n","        help='number of epochs (default: 100)')\n","    parser.add_argument('--lr', type=float, default=2e-4,\n","        help='learning rate for Adam optimizer (default: 2e-4)')\n","    parser.add_argument('--beta', type=float, default=1.0,\n","        help='contribution of commitment loss, between 0.1 and 2.0 (default: 1.0)')\n","\n","    # Miscellaneous\n","    parser.add_argument('--output-folder', type=str, default='vqvae',\n","        help='name of the output folder (default: vqvae)')\n","    parser.add_argument('--num-workers', type=int, default=mp.cpu_count() - 1,\n","        help='number of workers for trajectories sampling (default: {0})'.format(mp.cpu_count() - 1))\n","    parser.add_argument('--device', type=str, default='cpu',\n","        help='set the device (cpu or cuda, default: cpu)')\n","\n","    args = parser.parse_args()\n","\n","    # Create logs and models folder if they don't exist\n","    if not os.path.exists('./logs'):\n","        os.makedirs('./logs')\n","    if not os.path.exists('./models'):\n","        os.makedirs('./models')\n","    # Device\n","    args.device = torch.device(args.device\n","        if torch.cuda.is_available() else 'cpu')\n","    # Slurm\n","    if 'SLURM_JOB_ID' in os.environ:\n","        args.output_folder += '-{0}'.format(os.environ['SLURM_JOB_ID'])\n","    if not os.path.exists('./models/{0}'.format(args.output_folder)):\n","        os.makedirs('./models/{0}'.format(args.output_folder))\n","    args.steps = 0\n","\n","    main(args)\n"],"metadata":{"id":"kGv7XIWcI6cP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pixelcnnp_rior.py\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import json\n","from torchvision import transforms\n","from torchvision.utils import save_image, make_grid\n","\n","from modules import VectorQuantizedVAE, GatedPixelCNN\n","from datasets import MiniImagenet\n","\n","from tensorboardX import SummaryWriter\n","\n","def train(data_loader, model, prior, optimizer, args, writer):\n","    for images, labels in data_loader:\n","        with torch.no_grad():\n","            images = images.to(args.device)\n","            latents = model.encode(images)\n","            latents = latents.detach()\n","\n","        labels = labels.to(args.device)\n","        logits = prior(latents, labels)\n","        logits = logits.permute(0, 2, 3, 1).contiguous()\n","\n","        optimizer.zero_grad()\n","        loss = F.cross_entropy(logits.view(-1, args.k),\n","                               latents.view(-1))\n","        loss.backward()\n","\n","        # Logs\n","        writer.add_scalar('loss/train', loss.item(), args.steps)\n","\n","        optimizer.step()\n","        args.steps += 1\n","\n","def test(data_loader, model, prior, args, writer):\n","    with torch.no_grad():\n","        loss = 0.\n","        for images, labels in data_loader:\n","            images = images.to(args.device)\n","            labels = labels.to(args.device)\n","\n","            latents = model.encode(images)\n","            latents = latents.detach()\n","            logits = prior(latents, labels)\n","            logits = logits.permute(0, 2, 3, 1).contiguous()\n","            loss += F.cross_entropy(logits.view(-1, args.k),\n","                                    latents.view(-1))\n","\n","        loss /= len(data_loader)\n","\n","    # Logs\n","    writer.add_scalar('loss/valid', loss.item(), args.steps)\n","\n","    return loss.item()\n","\n","def main(args):\n","    writer = SummaryWriter('./logs/{0}'.format(args.output_folder))\n","    save_filename = './models/{0}/prior.pt'.format(args.output_folder)\n","\n","    if args.dataset in ['mnist', 'fashion-mnist', 'cifar10']:\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","        if args.dataset == 'mnist':\n","            # Define the train & test datasets\n","            train_dataset = datasets.MNIST(args.data_folder, train=True,\n","                download=True, transform=transform)\n","            test_dataset = datasets.MNIST(args.data_folder, train=False,\n","                transform=transform)\n","            num_channels = 1\n","        elif args.dataset == 'fashion-mnist':\n","            # Define the train & test datasets\n","            train_dataset = datasets.FashionMNIST(args.data_folder,\n","                train=True, download=True, transform=transform)\n","            test_dataset = datasets.FashionMNIST(args.data_folder,\n","                train=False, transform=transform)\n","            num_channels = 1\n","        elif args.dataset == 'cifar10':\n","            # Define the train & test datasets\n","            train_dataset = datasets.CIFAR10(args.data_folder,\n","                train=True, download=True, transform=transform)\n","            test_dataset = datasets.CIFAR10(args.data_folder,\n","                train=False, transform=transform)\n","            num_channels = 3\n","        valid_dataset = test_dataset\n","    elif args.dataset == 'miniimagenet':\n","        transform = transforms.Compose([\n","            transforms.RandomResizedCrop(128),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","        # Define the train, valid & test datasets\n","        train_dataset = MiniImagenet(args.data_folder, train=True,\n","            download=True, transform=transform)\n","        valid_dataset = MiniImagenet(args.data_folder, valid=True,\n","            download=True, transform=transform)\n","        test_dataset = MiniImagenet(args.data_folder, test=True,\n","            download=True, transform=transform)\n","        num_channels = 3\n","\n","    # Define the data loaders\n","    train_loader = torch.utils.data.DataLoader(train_dataset,\n","        batch_size=args.batch_size, shuffle=False,\n","        num_workers=args.num_workers, pin_memory=True)\n","    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n","        batch_size=args.batch_size, shuffle=False, drop_last=True,\n","        num_workers=args.num_workers, pin_memory=True)\n","    test_loader = torch.utils.data.DataLoader(test_dataset,\n","        batch_size=16, shuffle=True)\n","\n","    # Save the label encoder\n","    with open('./models/{0}/labels.json'.format(args.output_folder), 'w') as f:\n","        json.dump(train_dataset._label_encoder, f)\n","\n","    # Fixed images for Tensorboard\n","    fixed_images, _ = next(iter(test_loader))\n","    fixed_grid = make_grid(fixed_images, nrow=8, range=(-1, 1), normalize=True)\n","    writer.add_image('original', fixed_grid, 0)\n","\n","    model = VectorQuantizedVAE(num_channels, args.hidden_size_vae, args.k).to(args.device)\n","    with open(args.model, 'rb') as f:\n","        state_dict = torch.load(f)\n","        model.load_state_dict(state_dict)\n","    model.eval()\n","\n","    prior = GatedPixelCNN(args.k, args.hidden_size_prior,\n","        args.num_layers, n_classes=len(train_dataset._label_encoder)).to(args.device)\n","    optimizer = torch.optim.Adam(prior.parameters(), lr=args.lr)\n","\n","    best_loss = -1.\n","    for epoch in range(args.num_epochs):\n","        train(train_loader, model, prior, optimizer, args, writer)\n","        # The validation loss is not properly computed since\n","        # the classes in the train and valid splits of Mini-Imagenet\n","        # do not overlap.\n","        loss = test(valid_loader, model, prior, args, writer)\n","\n","        if (epoch == 0) or (loss < best_loss):\n","            best_loss = loss\n","            with open(save_filename, 'wb') as f:\n","                torch.save(prior.state_dict(), f)\n","\n","if __name__ == '__main__':\n","    import argparse\n","    import os\n","    import multiprocessing as mp\n","\n","    parser = argparse.ArgumentParser(description='PixelCNN Prior for VQ-VAE')\n","\n","    # General\n","    parser.add_argument('--data-folder', type=str,\n","        help='name of the data folder')\n","    parser.add_argument('--dataset', type=str,\n","        help='name of the dataset (mnist, fashion-mnist, cifar10, miniimagenet)')\n","    parser.add_argument('--model', type=str,\n","        help='filename containing the model')\n","\n","    # Latent space\n","    parser.add_argument('--hidden-size-vae', type=int, default=256,\n","        help='size of the latent vectors (default: 256)')\n","    parser.add_argument('--hidden-size-prior', type=int, default=64,\n","        help='hidden size for the PixelCNN prior (default: 64)')\n","    parser.add_argument('--k', type=int, default=512,\n","        help='number of latent vectors (default: 512)')\n","    parser.add_argument('--num-layers', type=int, default=15,\n","        help='number of layers for the PixelCNN prior (default: 15)')\n","\n","    # Optimization\n","    parser.add_argument('--batch-size', type=int, default=128,\n","        help='batch size (default: 128)')\n","    parser.add_argument('--num-epochs', type=int, default=100,\n","        help='number of epochs (default: 100)')\n","    parser.add_argument('--lr', type=float, default=3e-4,\n","        help='learning rate for Adam optimizer (default: 3e-4)')\n","\n","    # Miscellaneous\n","    parser.add_argument('--output-folder', type=str, default='prior',\n","        help='name of the output folder (default: prior)')\n","    parser.add_argument('--num-workers', type=int, default=mp.cpu_count() - 1,\n","        help='number of workers for trajectories sampling (default: {0})'.format(mp.cpu_count() - 1))\n","    parser.add_argument('--device', type=str, default='cpu',\n","        help='set the device (cpu or cuda, default: cpu)')\n","\n","    args = parser.parse_args()\n","\n","    # Create logs and models folder if they don't exist\n","    if not os.path.exists('./logs'):\n","        os.makedirs('./logs')\n","    if not os.path.exists('./models'):\n","        os.makedirs('./models')\n","    # Device\n","    args.device = torch.device(args.device\n","        if torch.cuda.is_available() else 'cpu')\n","    # Slurm\n","    if 'SLURM_JOB_ID' in os.environ:\n","        args.output_folder += '-{0}'.format(os.environ['SLURM_JOB_ID'])\n","    if not os.path.exists('./models/{0}'.format(args.output_folder)):\n","        os.makedirs('./models/{0}'.format(args.output_folder))\n","    args.steps = 0\n","\n","    main(args)\n"],"metadata":{"id":"5dG5xzl-Hcdm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eao9KkCQIV_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8ON_nKk4Jrm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PTWBhlaoJrpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nkUz7BSkIWBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qV8WgorCIWDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Yk38jaqmHcio"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Hrv1H8mwHcxn"},"execution_count":null,"outputs":[]}]}